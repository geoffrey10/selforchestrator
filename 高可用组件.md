1、背景
原有的MySQL高可用方案存在很多问题，无法满足业务对数据库的高可用要求。
2、主要功能
拓扑发现：能通过服务发现的方式识别并采集MySQL集群节点的主从配置、复制状态等基础信息
拓扑重构：能够识别多种复制类型：GTID、伪GTID、Binlog Server、重构复制拓扑只需将副本拖放至另一个主节点即可。通过各种命令行选项实现细粒度的控制。
故障恢复：定义了30+种故障模型，能够根据集群拓扑信息精准识别故障类型，针对不同的故障类型提供15种故障恢复执行计划，大大降低了故障恢复失败的概率。
界面支持：命令行、Web API、Web界面

orchestrator开源地址：https://github.com/openark/orchestrator
相关文档：https://github.com/openark/orchestrator/tree/master/docs

3、与MHA的差异

3.1 MHA自身问题
管理节点为单点，自身无高可用
通过SSH方式管理MySQL集群，存在安全风险。
无法管理1000+的MySQL集群规模，存在性能隐患。
故障探测单一，故障场景判断不丰富。
不支持定制化开发。可扩张性差。
社区不活跃，已不在维护。

3.2 orchestrator自身优势
自动发现MySQL拓扑结构，方案管理多套集群。并且在web上实时展示/
支持通过web页面的拖拽、命令行、API等多种方式灵活重构MySQL集群拓扑，变更复制关系。
支持30+主库异常场景，故障检测更全面，更客观。
支持多种切换方式：手工优雅切换，强制故障切换，自动切换。
主库检测异常发起切换动作，可通过Hooks自动以校本，与中间件联动。可扩张性强。
支持命令行、API、WEB界面等3种方式灵活使用。
golang语言编写，方便二次开发。

3.3 功能对比
3.3.1 差异对比


## 文档
故障恢复类型： https://github.com/openark/orchestrator/blob/master/docs/topology-recovery.md#automated-recovery
配置说明文档： https://github.com/openark/orchestrator/blob/master/docs/configuration.md
官方建议配置： https://github.com/openark/orchestrator/blob/master/docs/configuration-sample.md 
## 启动 
./orchestrator --config=/opt/soft/orchestrator/orchestrator.conf.json http >> $LOG_FILE 2>&1 &
## 线上环境 
10.135.1.15
10.135.2.150
10.135.1.71
## 其他   
mysql-sniffer抓包安装
yum install -y cmake libpcap-devel glib2-devel libnet-devel
git clone https://github.com/Qihoo360/mysql-sniffer.git
cd mysql-sniffer
mkdir proj
cd proj
cmake ../
make
cd proj/bin/
/bin/c++  -DENABLE_TCPREASM -O2 -Wall \
CMakeFiles/mysql-sniffer.dir/main.c.o \
CMakeFiles/mysql-sniffer.dir/mysql-dissector.c.o \
CMakeFiles/mysql-sniffer.dir/util.c.o \
CMakeFiles/mysql-sniffer.dir/session.cpp.o \
CMakeFiles/mysql-sniffer.dir/sniff-config.cpp.o \
CMakeFiles/mysql-sniffer.dir/sniff-log.cpp.o \
-o mysql-sniffer  \
-L/opt/soft/mysql-sniffer/lib \
-Wl,-rpath,/opt/soft/mysql-sniffer/lib \
-Wl,-Bstatic -lnidstcpreasm -lnet -lpcap -lglib-2.0 -lgthread-2.0 \
-Wl,-Bdynamic -lrt -lpthread

## 运行环境配置
### **1. meta信息设置：**
  "ReplicationLagQuery": "select absolute_lag from meta.heartbeat_view",
  "DetectClusterAliasQuery": "select ifnull(max(cluster_name), '') as cluster_alias from orch_meta.cluster where anchor=1",
  "DetectClusterDomainQuery": "select ifnull(max(cluster_domain), '') as cluster_domain from orch_meta.cluster where anchor=1",
    CREATE TABLE IF NOT EXISTS cluster (
    anchor TINYINT NOT NULL,
    cluster_name VARCHAR(128) CHARSET ascii NOT NULL DEFAULT '',
    cluster_domain VARCHAR(128) CHARSET ascii NOT NULL DEFAULT '',
    PRIMARY KEY (anchor)
    ) ENGINE=InnoDB DEFAULT CHARSET=utf8;
INSERT INTO cluster (anchor, cluster_name, cluster_domain) VALUES (1,'15967-1','test-newbrokerext.db.58dns.org	');
ON DUPLICATE KEY UPDATE  cluster_name=VALUES(cluster_name), cluster_domain=VALUES(cluster_domain)"

### **2. 授权**
后端数据库：
  "MySQLOrchestratorDatabase": "orchestrator",
  "MySQLOrchestratorUser": "orchestrator_rw",
  "MySQLOrchestratorPassword": "2041c28f72e6bbf6",  
通过cdb平台创建：
CREATE USER 'orchestrator_rw'@'10.%' IDENTIFIED BY '2041c28f72e6bbf6';
GRANT ALL ON orchestrator.* TO 'orchestrator_srv'@'10.%';
每个集群的拓扑发现：
  "MySQLTopologyUser": "dba",
  "MySQLTopologyPassword": "60e2e173396e360c",
  
    CREATE USER 'orchestrator'@'orc_host' IDENTIFIED BY 'orc_topology_password';
    GRANT SUPER, PROCESS, REPLICATION SLAVE, REPLICATION CLIENT, RELOAD ON *.* TO 'orchestrator'@'orc_host';
    GRANT SELECT ON orch_meta.* TO 'orchestrator'@'orc_host';

    GRANT SELECT ON ndbinfo.processes TO 'orchestrator'@'orc_host'; -- Only for NDB Cluster
    GRANT SELECT ON performance_schema.replication_group_members TO 'orchestrator'@'orc_host'; -- Only for Group Replication / InnoDB cluster

### **3. 配置http认证**
  "AuthenticationMethod": "basic",
  "HTTPAuthUser": "admin",
  "HTTPAuthPassword": "6133de347d29dfb0",
  "AuthUserHeader": "",
  "PowerAuthUsers": [
    "admin"
  ],
### **4. 配置拓扑发现 展示信息**
1/ 关闭 DiscoverByShowSlaveHosts
2/ 不要以hostname命令示例,ip展示
  "HostnameResolveMethod": "default",
  "MySQLHostnameResolveMethod": "none",
https://github.com/openark/orchestrator/blob/master/docs/configuration-discovery-resolve.md

### **5.API client**
export ORCHESTRATOR_API="http://10.135.1.15:3000/api http://10.135.2.150:3000/api http://10.135.1.71:3000/api"
orchestrator-client --auth admin:admin  -c clusters







### **故障测试：**
1. raft故障测试：
2024-08-20 11:52:07 DEBUG raft leader is 10.135.1.71:10008; state: Follower
2024/08/20 11:52:07 [WARN] raft: Heartbeat timeout from "10.135.1.71:10008" reached, starting election
2024/08/20 11:52:07 [INFO] raft: Node at 10.135.1.15:10008 [Candidate] entering Candidate state
2024/08/20 11:52:07 [ERR] raft: Failed to make RequestVote RPC to 10.135.1.71:10008: dial tcp 10.135.1.71:10008: connect: connection refused
2024/08/20 11:52:07 [DEBUG] raft: Votes needed: 2
2024/08/20 11:52:07 [DEBUG] raft: Vote granted from 10.135.1.15:10008. Tally: 1
2024/08/20 11:52:07 [DEBUG] raft-net: 10.135.1.15:10008 accepted connection from: 10.135.2.150:4084
2024/08/20 11:52:07 [INFO] raft: Duplicate RequestVote for same term: 28
2024/08/20 11:52:09 [INFO] raft: Node at 10.135.1.15:10008 [Follower] entering Follower state (Leader: "")
2024/08/20 11:52:09 [DEBUG] raft: Node 10.135.1.15:10008 updated peer set (2): [10.135.2.150:10008 10.135.1.71:10008 10.135.1.15:10008]
2024-08-20 11:52:09 DEBUG orchestrator/raft: applying command 24: leader-uri
2024/08/20 11:52:09 [DEBUG] raft-net: 10.135.1.15:10008 accepted connection from: 10.135.2.150:4100
2024-08-20 11:52:12 DEBUG raft leader is 10.135.2.150:10008; state: Follower
2024-08-20 11:52:13 DEBUG orchestrator/raft: applying command 25: request-health-report
2024-08-20 11:52:17 DEBUG raft leader is 10.135.2.150:10008; state: Follower
2024-08-20 11:52:22 DEBUG raft leader is 10.135.2.150:10008; state: Follower
2024-08-20 11:52:23 DEBUG orchestrator/raft: applying command 26: heartbeat
2024-08-20 11:52:23 DEBUG orchestrator/raft: applying command 27: request-health-report

2.
orchestrator-client --auth admin:admin -c graceful-master-takeover-auto -alias 15958-1 -d 10.176.85.104:15958